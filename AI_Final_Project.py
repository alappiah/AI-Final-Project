# -*- coding: utf-8 -*-
"""AI Final Project

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MhLJxtR34aLAX8px5suuOzTnkZSfhNyF
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import requests
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler, OrdinalEncoder
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.model_selection import train_test_split
import numpy as np
import xgboost as xgb
from sklearn.model_selection import cross_val_score, KFold
from sklearn.model_selection import GridSearchCV
import pickle
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
import matplotlib.pyplot as plt

# Fetching player and team data
fpl_url = 'https://fantasy.premierleague.com/api/bootstrap-static/'
response = requests.get(fpl_url)
data = response.json()

players_df = pd.DataFrame(data['elements'])
teams_df = pd.DataFrame(data['teams'])

# Fetching fixture data
fixtures_response = requests.get('https://fantasy.premierleague.com/api/fixtures/')
fixtures_data = fixtures_response.json()

# Convert fixtures data to DataFrame
fixtures_df = pd.DataFrame(fixtures_data)
fixtures_df.columns

player_team_fixture = pd.concat([players_df, teams_df,fixtures_df], axis=1)
#Removes duplicated comments
player_team_fixture = player_team_fixture.loc[:,~player_team_fixture.columns.duplicated()].copy()

team_id_to_name = teams_df.set_index('id')['name'].to_dict()
player_team_fixture['Home_team_name'] = player_team_fixture['team_h'].map(team_id_to_name)
player_team_fixture['Away_team_name'] = player_team_fixture['team_a'].map(team_id_to_name)

#Dropped columns with na's above a certain threshold
threshold = int(0.5 * 100)
df_elements = player_team_fixture.dropna(thresh = threshold, axis = 1)

#Dropping unecessary columns
player_team_fixture = player_team_fixture.drop(['first_name', 'second_name', 'photo', 'news', 'web_name', 'stats','points_per_game'], axis = 1)

#Gets the numeric data and imputes the empty rows
numeric_data = player_team_fixture.select_dtypes(include=['int64', 'float64'])
numeric_data_imputer = SimpleImputer(strategy='mean')
numeric_data_filled = numeric_data_imputer.fit_transform(numeric_data)
numeric_data_df = pd.DataFrame(numeric_data_filled, columns=numeric_data.columns)

#Gets the categorical data, imputes the data and encodes the data
categorical_data = player_team_fixture.select_dtypes(include=['object'])
categorical_data_imputer = SimpleImputer(strategy='most_frequent')
categorical_data_filled = categorical_data_imputer.fit_transform(categorical_data)
encoder = OrdinalEncoder()
categorical_data_encoded = encoder.fit_transform(categorical_data_filled)
categorical_data_df = pd.DataFrame(categorical_data_encoded, columns=categorical_data.columns)

player_team_fixture = pd.concat([numeric_data_df, categorical_data_df], axis=1)

corr_matrix = player_team_fixture.corr()

correlations = corr_matrix['total_points'].loc[corr_matrix['total_points'].abs() > 0.5].sort_values(ascending=False)
correlations

features = player_team_fixture[['bps','minutes', 'assists', 'goals_conceded', 'goals_scored']]

x = features
scaler = StandardScaler()
x = scaler.fit_transform(x)
y = player_team_fixture['total_points']

#random_state = Means we want to randomize the data
Xtrain,Xtest,Ytrain,Ytest = train_test_split(x,y,test_size=0.2, random_state=42)

rf_model = RandomForestRegressor()
rf_model.fit(Xtrain,Ytrain)
y_pred = rf_model.predict(Xtest)
print(f"""
Mean Absolute Error = {mean_absolute_error(y_pred,Ytest)},
Mean Squared Error = {mean_squared_error(y_pred,Ytest)},
Root Mean Squared Error = {np.sqrt(mean_squared_error(y_pred,Ytest))},
R2 Score = {r2_score(y_pred, Ytest)}
          """)

# xgb regressor to train model
xgb_model = xgb.XGBRegressor()
xgb_model.fit(Xtrain, Ytrain)
y_pred = xgb_model.predict(Xtest)
print(f"""
Mean Absolute Error = {mean_absolute_error(y_pred,Ytest)},
Mean Squared Error = {mean_squared_error(y_pred,Ytest)},
Root Mean Squared Error = {np.sqrt(mean_squared_error(y_pred,Ytest))},
R2 Score = {r2_score(y_pred, Ytest)}
          """)

gdr_model = GradientBoostingRegressor()
gdr_model.fit(Xtrain,Ytrain)
y_pred = gdr_model.predict(Xtest)
print(f"""
Mean Absolute Error = {mean_absolute_error(y_pred,Ytest)},
Mean Squared Error = {mean_squared_error(y_pred,Ytest)},
Root Mean Squared Error = {np.sqrt(mean_squared_error(y_pred,Ytest))},
R2 Score = {r2_score(y_pred, Ytest)}
          """)

# Define the model
model = Sequential([
    Dense(64, input_dim=Xtrain.shape[1], activation='relu'),
    Dense(32, activation='relu'),
    Dense(1)  # Output layer for regression
])

# Compile the model
model.compile(optimizer='adam', loss='mean_squared_error')

# Train the model
model.fit(Xtrain, Ytrain, epochs=50, batch_size=32, validation_split=0.2)

# Evaluate the model
loss = model.evaluate(Xtest, Ytest)
print(f'Test Loss: {loss:.4f}')

parameters = {
    'n_estimators': [100, 200, 300],
    'max_depth': [3, 4, 5],
    'learning_rate': [0.01, 0.1, 0.2],
}
grid_search = GridSearchCV(estimator = xgb_model, param_grid = parameters, cv=5, scoring='neg_mean_squared_error')

#Fit the grid search on the training data
grid_search.fit(Xtrain, Ytrain)
#Gets the best parameters
best_params = grid_search.best_params_
best_score = -grid_search.best_score_
best_rmse_xgb = np.sqrt(best_score)
print("Best Parameters:", best_params)
print("Best CV RMSE: %.4f" % best_score)
print("Best CV RMSE (XGB): %.4f" % best_rmse_xgb)

rf_param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [10, 20, 30],
}
rf_grid_search = GridSearchCV(estimator=rf_model, param_grid=rf_param_grid, cv=5, scoring='neg_mean_squared_error')
rf_grid_search.fit(Xtrain, Ytrain)

#Gets the best parameters
best_params = rf_grid_search.best_params_
best_score = -rf_grid_search.best_score_
best_rmse_rf = np.sqrt(best_score)
print("Best Parameters:", best_params)
print("Best CV RMSE: %.4f" % best_score)
print("Best CV RMSE (Random Forest): %.4f" % best_rmse_rf)

gdr_param_grid = {
    'n_estimators': [100, 200, 300],
    'learning_rate': [0.01, 0.1, 0.2],
    'max_depth': [3, 5, 7],
}
gdr_grid_search = GridSearchCV(estimator=gdr_model, param_grid=gdr_param_grid, cv=5, scoring='neg_mean_squared_error')
gdr_grid_search.fit(Xtrain, Ytrain)

best_params = gdr_grid_search.best_params_
best_score = -gdr_grid_search.best_score_
best_rmse_gdr = np.sqrt(best_score)
print("Best Parameters:", best_params)
print("Best CV RMSE: %.4f" % best_score)
print("Gradient Boosting Best CV RMSE: %.4f" % best_rmse_gdr)

#After looking at the Mean Squared Error and RMSE from grid search, gradient boosting appeared to be the best model
#It had the lowest Mean Squared Error and the lowest RMSE amongst the grid search

"""##Testing the trained model with input features from another dataset"""

test_data = pd.read_csv('/content/drive/MyDrive/merged_gwtest.csv')

numeric_data_test = test_data.select_dtypes(include=['int64', 'float64'])
numeric_data_test_imputer = SimpleImputer(strategy='mean')
numeric_data_test_filled = numeric_data_test_imputer.fit_transform(numeric_data_test)
numeric_data_test_df = pd.DataFrame(numeric_data_test_filled, columns=numeric_data_test.columns)

categorical_data_test = test_data.select_dtypes(include=['object'])
categorical_data_test_imputer = SimpleImputer(strategy='most_frequent')
categorical_data_test_filled = categorical_data_test_imputer.fit_transform(categorical_data_test)
encoder = OrdinalEncoder()
categorical_data_test_encoded = encoder.fit_transform(categorical_data_test_filled)
categorical_data_test_df = pd.DataFrame(categorical_data_test_encoded, columns=categorical_data_test.columns)

test_data = pd.concat([numeric_data_test_df, categorical_data_test_df], axis=1)

test_features = test_data[['bps','minutes', 'assists', 'goals_conceded', 'goals_scored']]
test_x = test_features
test_x = scaler.transform(test_x)
test_y = test_data['total_points']

Xtrain1,Xtest1,Ytrain1,Ytest1 = train_test_split(test_x,test_y,test_size=0.2, random_state=42)

gdr_model.fit(Xtrain1, Ytrain1)
test_ypred = gdr_model.predict(Xtest1)
print(f"""
Mean Absolute Error = {mean_absolute_error(test_ypred,Ytest1)},
Mean Squared Error = {mean_squared_error(test_ypred,Ytest1)},
Root Mean Squared Error = {np.sqrt(mean_squared_error(test_ypred,Ytest1))},
R2 Score = {r2_score(test_ypred,Ytest1)}
          """)

#A score of 0.9497 means that the random forest model fits the test data very well

plt.scatter(Ytest1, test_ypred)
plt.xlabel('Actual Values')
plt.ylabel('Predicted Values')
plt.title('Actual vs. Predicted Values')
plt.show()

# Save the model
with open('rf_model.pkl', 'wb') as model_file:
    pickle.dump(rf_model, model_file)

# Save the model
with open('gdr_model.pkl', 'wb') as model_file:
    pickle.dump(gdr_model, model_file)

